{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6WDvajSqIDs"
   },
   "source": [
    "# APS360 Project\n",
    "\n",
    "1. Load and split data for training, validation and testing\n",
    "2. Creating Generator\n",
    "3. Creating Discriminator\n",
    "4. Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJxMgWGNqID2"
   },
   "source": [
    "## Part A. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_DIR = \"data/train\"\n",
    "VAL_DIR = \"data/val\"\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.0002\n",
    "LAMBDA_IDENTITY = 0.0  # loss weight for identity loss\n",
    "LAMBDA_CYCLE = 10\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 50\n",
    "PM = 64\n",
    "LOAD_MODEL = True\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_GENERATOR_ANIME = \"models/gen_anime\"\n",
    "CHECKPOINT_GENERATOR_HUMAN = \"models/gen_human\"\n",
    "CHECKPOINT_DISCRIMINATOR_ANIME = \"models/disc_anime\"\n",
    "CHECKPOINT_DISCRIMINATOR_HUMAN = \"models/disc_human\"\n",
    "\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=128, height=128),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    additional_targets={\"image0\": \"image\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Helper Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Data Loading\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "class ImageBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), \"Empty buffer\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)\n",
    "\n",
    "\n",
    "class AnimeHumanImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, fname))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = np.array(image)  # Convert to NumPy array for albumentations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, 0  # Since there are no labels, we can return a dummy label (0)\n",
    "    \n",
    "\n",
    "def create_subset_dataset(original_dir, subset_dir, subset_size=100):\n",
    "    \"\"\"\n",
    "    Creates a subset of the original dataset.\n",
    "    \n",
    "    Args:\n",
    "        original_dir (str): Path to the original dataset directory.\n",
    "        subset_dir (str): Path to the subset dataset directory.\n",
    "        subset_size (int): Number of images to include in the subset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(subset_dir):\n",
    "        os.makedirs(subset_dir)\n",
    "    else:\n",
    "        # Clear the subset directory\n",
    "        for filename in os.listdir(subset_dir):\n",
    "            file_path = os.path.join(subset_dir, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "    \n",
    "    # Get all image paths\n",
    "    image_paths = [os.path.join(original_dir, fname) for fname in os.listdir(original_dir) if os.path.isfile(os.path.join(original_dir, fname))]\n",
    "    \n",
    "    # Shuffle and select a subset\n",
    "    np.random.seed(1000)\n",
    "    np.random.shuffle(image_paths)\n",
    "    subset_paths = image_paths[:subset_size]\n",
    "    \n",
    "    # Copy images to the subset directory\n",
    "    for img_path in subset_paths:\n",
    "        shutil.copy(img_path, subset_dir)\n",
    "\n",
    "    print(f\"Subset created with {len(subset_paths)} images in {subset_dir}\")\n",
    "\n",
    "def get_data_loader(data_dir, batch_size, image_size=(128, 128), subset_size=None):\n",
    "   # Define the transformation\n",
    "    transform = A.Compose([\n",
    "        A.Resize(width=image_size[0], height=image_size[1]),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    # Verify the path\n",
    "    assert os.path.exists(data_dir), f\"Directory not found: {data_dir}\"\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = AnimeHumanImageDataset(root_dir=data_dir, transform=transform)\n",
    "\n",
    "    # Use a subset of the dataset if specified\n",
    "    if subset_size:\n",
    "        num_images = len(dataset)\n",
    "        indices = list(range(num_images))\n",
    "        np.random.seed(1000)\n",
    "        np.random.shuffle(indices)\n",
    "        subset_indices = indices[:subset_size]\n",
    "        subset_sampler = SubsetRandomSampler(subset_indices)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, sampler=subset_sampler, num_workers=1)\n",
    "        return loader\n",
    "\n",
    "    # Function to split dataset indices\n",
    "    def split_indices(dataset):\n",
    "        num_images = len(dataset)\n",
    "        indices = list(range(num_images))\n",
    "        np.random.seed(1000)  # Setting a seed for reproducibility\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_split = int(0.7 * num_images)\n",
    "        val_split = int(0.15 * num_images)\n",
    "\n",
    "        train_indices = indices[:train_split]\n",
    "        val_indices = indices[train_split:train_split + val_split]\n",
    "        test_indices = indices[train_split + val_split:]\n",
    "\n",
    "        return train_indices, val_indices, test_indices\n",
    "\n",
    "    # Get split indices for the dataset\n",
    "    train_indices, val_indices, test_indices = split_indices(dataset)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, num_workers=1)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=1)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def verify_splits_and_check_overlaps(train_loader_A, val_loader_A, test_loader_A, train_loader_B, val_loader_B, test_loader_B):\n",
    "    # Verify the split sizes for dataset A\n",
    "    total_examples_A = len(train_loader_A.sampler) + len(val_loader_A.sampler) + len(test_loader_A.sampler)\n",
    "    train_size_A = len(train_loader_A.sampler)\n",
    "    val_size_A = len(val_loader_A.sampler)\n",
    "    test_size_A = len(test_loader_A.sampler)\n",
    "\n",
    "    train_proportion_A = train_size_A / total_examples_A\n",
    "    val_proportion_A = val_size_A / total_examples_A \n",
    "    test_proportion_A = test_size_A / total_examples_A\n",
    "\n",
    "    print(f\"Dataset A - Total examples: {total_examples_A}\")\n",
    "    print(f\"Dataset A - Train examples: {train_size_A} ({train_proportion_A:.2%})\")\n",
    "    print(f\"Dataset A - Validation examples: {val_size_A} ({val_proportion_A:.2%})\")\n",
    "    print(f\"Dataset A - Test examples: {test_size_A} ({test_proportion_A:.2%})\")\n",
    "\n",
    "    # Verify the split sizes for dataset B\n",
    "    total_examples_B = len(train_loader_B.sampler) + len(val_loader_B.sampler) + len(test_loader_B.sampler)\n",
    "    train_size_B = len(train_loader_B.sampler)\n",
    "    val_size_B = len(val_loader_B.sampler)\n",
    "    test_size_B = len(test_loader_B.sampler)\n",
    "\n",
    "    train_proportion_B = train_size_B / total_examples_B\n",
    "    val_proportion_B = val_size_B / total_examples_B \n",
    "    test_proportion_B = test_size_B / total_examples_B\n",
    "\n",
    "    print(f\"Dataset B - Total examples: {total_examples_B}\")\n",
    "    print(f\"Dataset B - Train examples: {train_size_B} ({train_proportion_B:.2%})\")\n",
    "    print(f\"Dataset B - Validation examples: {val_size_B} ({val_proportion_B:.2%})\")\n",
    "    print(f\"Dataset B - Test examples: {test_size_B} ({test_proportion_B:.2%})\")\n",
    "\n",
    "    # Check for overlaps in dataset A\n",
    "    train_indices_A = list(train_loader_A.sampler.indices)\n",
    "    val_indices_A = list(val_loader_A.sampler.indices)\n",
    "    test_indices_A = list(test_loader_A.sampler.indices)\n",
    "    print(\"Checking overlaps for dataset A...\")\n",
    "    check_for_overlaps(train_indices_A, val_indices_A, test_indices_A)\n",
    "\n",
    "    # Check for overlaps in dataset B\n",
    "    train_indices_B = list(train_loader_B.sampler.indices)\n",
    "    val_indices_B = list(val_loader_B.sampler.indices)\n",
    "    test_indices_B = list(test_loader_B.sampler.indices)\n",
    "    print(\"Checking overlaps for dataset B...\")\n",
    "    check_for_overlaps(train_indices_B, val_indices_B, test_indices_B)\n",
    "\n",
    "def check_for_overlaps(train_indices, val_indices, test_indices):\n",
    "    # Convert indices to sets\n",
    "    train_indices_set = set(train_indices)\n",
    "    val_indices_set = set(val_indices)\n",
    "    test_indices_set = set(test_indices)\n",
    "\n",
    "    # Check for overlaps\n",
    "    train_val_overlap = train_indices_set.intersection(val_indices_set)\n",
    "    train_test_overlap = train_indices_set.intersection(test_indices_set)\n",
    "    val_test_overlap = val_indices_set.intersection(test_indices_set)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Train/Validation Overlap: {len(train_val_overlap)} indices\")\n",
    "    print(f\"Train/Test Overlap: {len(train_test_overlap)} indices\")\n",
    "    print(f\"Validation/Test Overlap: {len(val_test_overlap)} indices\")\n",
    "\n",
    "    if not train_val_overlap and not train_test_overlap and not val_test_overlap:\n",
    "        print(\"No overlaps found between train, validation, and test sets.\")\n",
    "    else:\n",
    "        print(\"Overlaps detected. Please check the data splitting logic.\")\n",
    "\n",
    "###############################################################################\n",
    "# Training\n",
    "import glob\n",
    "\n",
    "def evaluate(generator, discriminator, loader, criterion, device):\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Generate fake images\n",
    "            fake_images = generator(inputs)\n",
    "            outputs = discriminator(fake_images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, torch.ones_like(outputs).to(device))\n",
    "            total_loss += loss.item()\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(f\"=> Saving checkpoint to {filename}\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_latest_checkpoint(model, optimizer, checkpoint_prefix, lr, batch_size):\n",
    "    checkpoint_files = glob.glob(f\"{checkpoint_prefix}_lr{lr}_bs{batch_size}_epoch_*.pth.tar\")\n",
    "    if not checkpoint_files:\n",
    "        print(f\"=> No checkpoint found for {checkpoint_prefix}. Starting from scratch.\")\n",
    "        return 0\n",
    "\n",
    "    # Extract the latest epoch number from the checkpoint file names\n",
    "    latest_epoch = max([int(file.split('_')[-1].split('.')[0]) for file in checkpoint_files])\n",
    "    checkpoint_file = f\"{checkpoint_prefix}_lr{lr}_bs{batch_size}_epoch_{latest_epoch}.pth.tar\"\n",
    "\n",
    "    print(f\"=> Loading checkpoint {checkpoint_file}\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    return latest_epoch + 1\n",
    "\n",
    "def get_model_name(model_type, epoch, learning_rate, batch_size):\n",
    "    return f\"models/{model_type}_lr{learning_rate}_bs{batch_size}_epoch_{epoch}.pth.tar\"\n",
    "\n",
    "def list_checkpoints(checkpoint_prefix):\n",
    "    checkpoint_pattern = os.path.join(f\"{checkpoint_prefix}_epoch_*.pth.tar\")\n",
    "    checkpoint_files = glob.glob(checkpoint_pattern)\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(f\"No checkpoint found for prefix: {checkpoint_prefix}\")\n",
    "    else:\n",
    "        print(f\"Checkpoints found for prefix {checkpoint_prefix}:\")\n",
    "        for checkpoint in checkpoint_files:\n",
    "            print(checkpoint)\n",
    "            \n",
    "def save_losses(train_loss_anime_history, train_loss_human_history, val_loss_anime_history, val_loss_human_history, path_prefix):\n",
    "    np.savetxt(f\"{path_prefix}_train_loss_anime.csv\", train_loss_anime_history)\n",
    "    np.savetxt(f\"{path_prefix}_train_loss_human.csv\", train_loss_human_history)\n",
    "    np.savetxt(f\"{path_prefix}_val_loss_anime.csv\", val_loss_anime_history)\n",
    "    np.savetxt(f\"{path_prefix}_val_loss_human.csv\", val_loss_human_history)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Training Curve\n",
    "def plot_training_curve(path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_loss_anime_file = f\"{path}_train_loss_anime.csv\"\n",
    "    train_loss_human_file = f\"{path}_train_loss_human.csv\"\n",
    "    val_loss_anime_file = f\"{path}_val_loss_anime.csv\"\n",
    "    val_loss_human_file = f\"{path}_val_loss_human.csv\"\n",
    "\n",
    "    train_loss_anime = np.loadtxt(train_loss_anime_file)\n",
    "    train_loss_human = np.loadtxt(train_loss_human_file)\n",
    "    val_loss_anime = np.loadtxt(val_loss_anime_file)\n",
    "    val_loss_human = np.loadtxt(val_loss_human_file)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Train vs Validation Loss (Anime)\")\n",
    "    n = len(train_loss_anime)  # number of epochs\n",
    "    plt.plot(range(1, n + 1), train_loss_anime, label=\"Train Anime\")\n",
    "    plt.plot(range(1, n + 1), val_loss_anime, label=\"Validation Anime\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Train vs Validation Loss (Human)\")\n",
    "    plt.plot(range(1, n + 1), train_loss_human, label=\"Train Human\")\n",
    "    plt.plot(range(1, n + 1), val_loss_human, label=\"Validation Human\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiDuQaAh56sT"
   },
   "source": [
    "### 1. Data Loading and Splitting [5 pt]\n",
    "\n",
    "Download the anonymized data provided on Quercus. To allow you to get a heads start on this project we will provide you with sample data from previous years. Split the data into training, validation, and test sets.\n",
    "\n",
    "Note: Data splitting is not as trivial in this lab. We want our test set to closely resemble the setting in which\n",
    "our model will be used. In particular, our test set should contain hands that are never seen in training!\n",
    "\n",
    "Explain how you split the data, either by describing what you did, or by showing the code that you used.\n",
    "Justify your choice of splitting strategy. How many training, validation, and test images do you have?\n",
    "\n",
    "For loading the data, you can use plt.imread as in Lab 1, or any other method that you choose. You may find\n",
    "torchvision.datasets.ImageFolder helpful. (see https://pytorch.org/docs/stable/torchvision/datasets.html?highlight=image%20folder#torchvision.datasets.ImageFolder\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use (1000k photos) smaller subsets for quick testing\n",
    "# create_subset_dataset('/home/jempio/documents/aps360-project/data/dataSetA_10k', 'data/subset_dataSetA', subset_size=1000)\n",
    "# create_subset_dataset('/home/jempio/documents/aps360-project/data/dataSetB_10k', 'data/subset_dataSetB', subset_size=1000)\n",
    "\n",
    "# # Load data from the subsets\n",
    "# train_loader_A, val_loader_A, test_loader_A = get_data_loader('data/subset_dataSetA', BATCH_SIZE, image_size=(128, 128))\n",
    "# train_loader_B, val_loader_B, test_loader_B = get_data_loader('data/subset_dataSetB', BATCH_SIZE, image_size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WBrH5kBqRLa6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load data from the full datasets\n",
    "train_loader_A, val_loader_A, test_loader_A = get_data_loader('/home/jempio/documents/aps360-project/data/dataSetA_10k', BATCH_SIZE, image_size=(128, 128))\n",
    "train_loader_B, val_loader_B, test_loader_B = get_data_loader('/home/jempio/documents/aps360-project/data/dataSetB_10k', BATCH_SIZE, image_size=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verifying Split and Overlap ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overlaps\n",
    "verify_splits_and_check_overlaps(train_loader_A, val_loader_A, test_loader_A, train_loader_B, val_loader_B, test_loader_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualize Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Taken from Lab 2\n",
    "def visualize_data_loader(data_loader, title, num_images=5):\n",
    "    k = 0\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for images, labels in data_loader:\n",
    "        for i in range(images.size(0)):  \n",
    "            if k >= num_images:\n",
    "                break\n",
    "            image = images[i]\n",
    "            img = np.transpose(image.numpy(), [1, 2, 0])\n",
    "            img = img / 2 + 0.5\n",
    "            plt.subplot(3, 5, k+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img)\n",
    "            k += 1\n",
    "        if k >= num_images:\n",
    "            break\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize dataset A\n",
    "visualize_data_loader(train_loader_A, title=\"Dataset A - Train Loader\")\n",
    "visualize_data_loader(val_loader_A, title=\"Dataset A - Validation Loader\")\n",
    "visualize_data_loader(test_loader_A, title=\"Dataset A - Test Loader\")\n",
    "\n",
    "# Visualize dataset B\n",
    "visualize_data_loader(train_loader_B, title=\"Dataset B - Train Loader\")\n",
    "visualize_data_loader(val_loader_B, title=\"Dataset B - Validation Loader\")\n",
    "visualize_data_loader(test_loader_B, title=\"Dataset B - Test Loader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Define the reusable convolutional block\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, is_downsampling: bool = True, add_activation: bool = True, **kwargs):\n",
    "        super().__init__()\n",
    "        if is_downsampling:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the residual block using the reusable convolutional block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvolutionalBlock(channels, channels, add_activation=True, kernel_size=3, padding=1, is_downsampling=False),\n",
    "            ConvolutionalBlock(channels, channels, add_activation=False, kernel_size=3, padding=1, is_downsampling=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, pm=64, num_residuals=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial Layer\n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=pm * 1, kernel_size=7, stride=1, padding=0, bias=True),\n",
    "            nn.InstanceNorm2d(pm * 1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Downsampling\n",
    "        self.downsampling_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=pm * 1, out_channels=pm * 2, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.InstanceNorm2d(pm * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=pm * 2, out_channels=pm * 4, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.InstanceNorm2d(pm * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Residual Blocks\n",
    "        self.residual_layers = nn.Sequential(\n",
    "            *[ResidualBlock(channels=pm * 4) for _ in range(num_residuals)]\n",
    "        )\n",
    "\n",
    "        # Upsampling\n",
    "        self.upsampling_layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=pm * 4, out_channels=pm * 2, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "            nn.InstanceNorm2d(pm * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=pm * 2, out_channels=pm * 1, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "            nn.InstanceNorm2d(pm * 1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Final Layer\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(pm, out_channels, kernel_size=7, stride=1, padding=0, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.initial_layer(x)\n",
    "        x = self.downsampling_layers(x)\n",
    "        x = self.residual_layers(x)\n",
    "        x = self.upsampling_layers(x)\n",
    "        return self.last_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reusable block\n",
    "class ConvInstanceNormLeakyReLUBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=4,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MiniBatchDiscrimination(nn.Module):\n",
    "#     def __init__(self, in_features, out_features, kernel_dims=5):\n",
    "#         super(MiniBatchDiscrimination, self).__init__()\n",
    "#         self.out_features = out_features\n",
    "#         self.T = nn.Parameter(torch.randn(in_features, out_features, kernel_dims))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "#         x = x.view(batch_size, -1)  # Flatten the tensor\n",
    "\n",
    "#         # Compute matrix multiplication\n",
    "#         M = x.mm(self.T.view(x.size(1), -1))\n",
    "#         M = M.view(batch_size, self.out_features, self.T.size(2))\n",
    "\n",
    "#         # Compute L1 distance between each pair of samples in the batch\n",
    "#         out = M.unsqueeze(0) - M.unsqueeze(1)\n",
    "#         out = torch.exp(-torch.sum(torch.abs(out), dim=3))\n",
    "\n",
    "#         # Sum over the kernel dimensions\n",
    "#         out = out.sum(dim=2)\n",
    "\n",
    "#         return torch.cat([x, out], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator using the reusable block\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, pm=64):\n",
    "        super().__init__()\n",
    "        features = [pm * 1, pm * 2, pm * 4, pm * 8]\n",
    "        \n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                features[0],\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\",\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                ConvInstanceNormLeakyReLUBlock(\n",
    "                    in_channels,\n",
    "                    feature,\n",
    "                    stride=2\n",
    "                )\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        # Adjust the last block to match the original structure\n",
    "        layers.append(\n",
    "            ConvInstanceNormLeakyReLUBlock(\n",
    "                in_channels,\n",
    "                features[-1],\n",
    "                stride=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Final layer\n",
    "        layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=features[-1],\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_layer(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "def denormalize(tensor):\n",
    "    return tensor * 0.5 + 0.5\n",
    "\n",
    "def train_fn(disc_human, disc_anime, gen_anime, gen_human, train_loader_A, train_loader_B, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler, num_epochs=1, start_epoch=0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loss_anime_history = []\n",
    "    train_loss_human_history = []\n",
    "    val_loss_anime_history = []\n",
    "    val_loss_human_history = []\n",
    "\n",
    "    # Ensure the outputs directory exists\n",
    "    os.makedirs('outputs/A', exist_ok=True)\n",
    "    os.makedirs('outputs/B', exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_epoch = start_epoch + epoch\n",
    "        print(f\"Epoch [{current_epoch + 1}/{start_epoch + num_epochs}]\")\n",
    "        epoch_loop = tqdm(zip(train_loader_A, train_loader_B), total=min(len(train_loader_A), len(train_loader_B)), leave=True, desc=\"Iteration\")\n",
    "\n",
    "        total_train_loss_anime = 0.0\n",
    "        total_train_loss_human = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        H_reals = 0\n",
    "        H_fakes = 0\n",
    "\n",
    "        # Learning rate decay for over 100 epochs\n",
    "        if epoch > 100:\n",
    "            for param_group in opt_gen.param_groups:\n",
    "                param_group['lr'] *= 0.993\n",
    "            for param_group in opt_disc.param_groups:\n",
    "                param_group['lr'] *= 0.993\n",
    "\n",
    "        for idx, (a_real, b_real) in enumerate(epoch_loop):\n",
    "            a_real = a_real[0]  # Remove label from imagefolder\n",
    "            b_real = b_real[0]\n",
    "            a_real = a_real.to(device)\n",
    "            b_real = b_real.to(device)\n",
    "                \n",
    "            # A: faces\n",
    "            # B: anime\n",
    "            # Generators\n",
    "            \n",
    "            # -----------------------------\n",
    "            # Anime Cycle: B -> A' -> B^ === b_real -> a_fake -> b-recon\n",
    "            # -----------------------------\n",
    "            \n",
    "            opt_gen.zero_grad()\n",
    "            a_fake = gen_anime(b_real)\n",
    "            a_dis_pred = disc_anime(a_fake)\n",
    "            \n",
    "            # Create a truth label\n",
    "            label_1 = torch.ones(a_dis_pred.size(), device=device)\n",
    "            # Generator MSE loss\n",
    "            b_gen_loss = mse(a_dis_pred, label_1)\n",
    "            \n",
    "            b_recon = gen_human(a_fake)\n",
    "            b_cycle_loss = l1(b_recon, b_real) * LAMBDA_CYCLE\n",
    "            \n",
    "            b_idt = gen_anime(a_real)\n",
    "            b_idt_losses = l1(b_idt, a_real) * LAMBDA_CYCLE * LAMBDA_IDENTITY\n",
    "            \n",
    "            b_total_loss = b_gen_loss + b_cycle_loss + b_idt_losses\n",
    "            b_total_loss.backward(retain_graph=True)\n",
    "            opt_gen.step()\n",
    "            \n",
    "            # -----------------------------\n",
    "            # Selfie Cycle: A -> B' -> A^\n",
    "            # -----------------------------\n",
    "            \n",
    "            opt_gen.zero_grad()\n",
    "            b_fake = gen_human(a_real)\n",
    "            b_dis_pred = disc_human(b_fake)\n",
    "            # Create a truth label\n",
    "            label_1 = torch.ones(b_dis_pred.size(), device=device)\n",
    "            a_gen_loss = mse(b_dis_pred, label_1)\n",
    "            \n",
    "            a_recon = gen_anime(b_fake)\n",
    "            a_cycle_loss = l1(a_recon, a_real) * LAMBDA_CYCLE\n",
    "            \n",
    "            a_idt = gen_human(b_real)\n",
    "            a_idt_losses = l1(a_idt, b_real) * LAMBDA_CYCLE * LAMBDA_IDENTITY\n",
    "            \n",
    "            a_total_loss = a_gen_loss + a_cycle_loss + a_idt_losses\n",
    "            a_total_loss.backward(retain_graph=True)\n",
    "            opt_gen.step()\n",
    "            \n",
    "            # -----------------------------\n",
    "            # B Discriminators\n",
    "            # -----------------------------\n",
    "            opt_disc.zero_grad()\n",
    "            \n",
    "            # Real\n",
    "            b_real_dis = disc_anime(a_real)\n",
    "            \n",
    "            dlabel_1 = torch.ones(b_real_dis.size(), device=device)\n",
    "            b_dis_real_loss = mse(b_real_dis, dlabel_1)\n",
    "            \n",
    "            # Fake\n",
    "            b_fake_dis = disc_anime(a_fake.detach())  # Detach to avoid computing gradients\n",
    "            dlabel_0 = torch.zeros(b_fake_dis.size(), device=device)\n",
    "            b_dis_fake_loss = mse(b_fake_dis, dlabel_0)\n",
    "            \n",
    "            # Step optimizer and backprop\n",
    "            b_dis_loss = (b_dis_real_loss + b_dis_fake_loss) * 0.5\n",
    "            b_dis_loss.backward()\n",
    "            opt_disc.step()\n",
    "            \n",
    "            # -----------------------------\n",
    "            # A Discriminators\n",
    "            # -----------------------------\n",
    "            opt_disc.zero_grad()\n",
    "            \n",
    "            # Real\n",
    "            a_real_dis = disc_human(b_real)\n",
    "            \n",
    "            dlabel_1 = torch.ones(a_real_dis.size(), device=device)\n",
    "            a_dis_real_loss = mse(a_real_dis, dlabel_1)\n",
    "            \n",
    "            # Fake\n",
    "            a_fake_dis = disc_human(b_fake.detach())  # Detach to avoid computing gradients\n",
    "            dlabel_0 = torch.zeros(a_fake_dis.size(), device=device)\n",
    "            a_dis_fake_loss = mse(a_fake_dis, dlabel_0)\n",
    "\n",
    "            # Accumulate real and fake mean values for human\n",
    "            H_reals += a_real_dis.mean().item()\n",
    "            H_fakes += a_fake_dis.mean().item()\n",
    "            \n",
    "            # Step optimizer and backprop\n",
    "            a_dis_loss = (a_dis_real_loss + a_dis_fake_loss) * 0.5\n",
    "            a_dis_loss.backward()\n",
    "            opt_disc.step()\n",
    "            \n",
    "             # Save images more frequently\n",
    "            if idx % 3500 == 0:  # Save every 3500 iterations\n",
    "                # Save Human to Anime (A) transformation images\n",
    "                real_anime = denormalize(a_real)  # Denormalize the images\n",
    "                fake_human = denormalize(b_fake)\n",
    "                cycle_anime = denormalize(a_recon)\n",
    "                B_grid = torch.cat((real_anime, fake_human, cycle_anime), dim=0)\n",
    "                filename = f\"human_to_anime_epoch_{current_epoch}_{idx}.png\"\n",
    "                save_path = os.path.join('outputs/B', filename)\n",
    "                save_image(B_grid, save_path, nrow=3, normalize=True)\n",
    "\n",
    "                # Save Anime to Human (B) transformation images\n",
    "                real_human = denormalize(b_real)  # Denormalize the images\n",
    "                fake_anime = denormalize(a_fake)\n",
    "                cycle_human = denormalize(b_recon)\n",
    "                A_grid = torch.cat((real_human, fake_anime, cycle_human), dim=0)\n",
    "                filename = f\"anime_to_human_epoch_{current_epoch}_{idx}.png\"\n",
    "                save_path = os.path.join('outputs/A', filename)\n",
    "                save_image(A_grid, save_path, nrow=3, normalize=True)\n",
    "\n",
    "            # Update iteration progress bar\n",
    "            epoch_loop.set_postfix(D_loss=(a_dis_loss + b_dis_loss).item(), G_loss=(a_total_loss + b_total_loss).item(), H_real=H_reals / (idx + 1), H_fake=H_fakes / (idx + 1))\n",
    "            epoch_loop.update(1)\n",
    "\n",
    "            # Accumulate training losses\n",
    "            total_train_loss_anime += b_total_loss.item()\n",
    "            total_train_loss_human += a_total_loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "        epoch_loop.close()\n",
    "\n",
    "        # Calculate average training losses\n",
    "        avg_train_loss_anime = total_train_loss_anime / total_batches\n",
    "        avg_train_loss_human = total_train_loss_human / total_batches\n",
    "\n",
    "        # Evaluate models\n",
    "        val_loss_anime = evaluate(gen_anime, disc_anime, val_loader_A, mse, device)\n",
    "        val_loss_human = evaluate(gen_human, disc_human, val_loader_B, mse, device)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {current_epoch + 1}: Train Loss Anime: {avg_train_loss_anime}, Train Loss Human: {avg_train_loss_human}, Validation Loss Anime: {val_loss_anime}, Validation Loss Human: {val_loss_human}\")\n",
    "\n",
    "        # Save checkpoint at the end of each epoch\n",
    "        save_checkpoint(gen_anime, opt_gen, filename=get_model_name(\"gen_anime\", current_epoch, LEARNING_RATE, BATCH_SIZE))\n",
    "        save_checkpoint(gen_human, opt_gen, filename=get_model_name(\"gen_human\", current_epoch, LEARNING_RATE, BATCH_SIZE))\n",
    "        save_checkpoint(disc_anime, opt_disc, filename=get_model_name(\"disc_anime\", current_epoch, LEARNING_RATE, BATCH_SIZE))\n",
    "        save_checkpoint(disc_human, opt_disc, filename=get_model_name(\"disc_human\", current_epoch, LEARNING_RATE, BATCH_SIZE))\n",
    "\n",
    "        print(f\"Epoch {current_epoch + 1} checkpoint saved.\")\n",
    "\n",
    "        # Save training and validation losses\n",
    "        train_loss_anime_history.append(avg_train_loss_anime)\n",
    "        train_loss_human_history.append(avg_train_loss_human)\n",
    "        val_loss_anime_history.append(val_loss_anime)\n",
    "        val_loss_human_history.append(val_loss_human)\n",
    "\n",
    "    save_losses(train_loss_anime_history, train_loss_human_history, val_loss_anime_history, val_loss_human_history, \n",
    "                get_model_name(\"loss\", current_epoch, LEARNING_RATE, BATCH_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "\n",
    "gen_anime = Generator(in_channels=3, out_channels=3, pm=PM, num_residuals=6).to(DEVICE)\n",
    "gen_human = Generator(in_channels=3, out_channels=3, pm=PM, num_residuals=6).to(DEVICE)\n",
    "disc_anime = Discriminator(in_channels=3, pm=PM).to(DEVICE)\n",
    "disc_human = Discriminator(in_channels=3, pm=PM).to(DEVICE)\n",
    "\n",
    "opt_gen = optim.Adam(\n",
    "    list(gen_anime.parameters()) + list(gen_human.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "opt_disc = optim.Adam(\n",
    "    list(disc_anime.parameters()) + list(disc_human.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "l1 = nn.L1Loss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "d_scaler = GradScaler()\n",
    "g_scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# list_checkpoints(\"models/gen_anime\")\n",
    "# list_checkpoints(\"models/gen_human\")\n",
    "# list_checkpoints(\"models/disc_anime\")\n",
    "# list_checkpoints(\"models/disc_human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "        start_epoch = load_latest_checkpoint(gen_anime, opt_gen, CHECKPOINT_GENERATOR_ANIME, LEARNING_RATE, BATCH_SIZE)\n",
    "        start_epoch = load_latest_checkpoint(gen_human, opt_gen, CHECKPOINT_GENERATOR_HUMAN, LEARNING_RATE, BATCH_SIZE)\n",
    "        start_epoch = load_latest_checkpoint(disc_anime, opt_disc, CHECKPOINT_DISCRIMINATOR_ANIME, LEARNING_RATE, BATCH_SIZE)\n",
    "        start_epoch = load_latest_checkpoint(disc_human, opt_disc, CHECKPOINT_DISCRIMINATOR_HUMAN, LEARNING_RATE, BATCH_SIZE)\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if LOAD_MODEL:\n",
    "#         start_epoch = load_latest_checkpoint(gen_anime, opt_gen, CHECKPOINT_GENERATOR_ANIME, LEARNING_RATE, BATCH_SIZE)\n",
    "#         start_epoch = load_latest_checkpoint(gen_human, opt_gen, CHECKPOINT_GENERATOR_HUMAN, LEARNING_RATE, BATCH_SIZE)\n",
    "#         start_epoch = load_latest_checkpoint(disc_anime, opt_disc, CHECKPOINT_DISCRIMINATOR_ANIME, LEARNING_RATE, BATCH_SIZE)\n",
    "#         start_epoch = load_latest_checkpoint(disc_human, opt_disc, CHECKPOINT_DISCRIMINATOR_HUMAN, LEARNING_RATE, BATCH_SIZE)\n",
    "#         print(f\"Resuming from epoch {start_epoch}\")\n",
    "# else:\n",
    "#     start_epoch = 0\n",
    "start_epoch = 0\n",
    "\n",
    "num_epochs = 50  # Total number of epochs to run\n",
    "remaining_epochs = num_epochs - start_epoch\n",
    "print(f\"Starting training for {remaining_epochs} epochs from epoch {start_epoch + 1}\")\n",
    "\n",
    " \n",
    "train_fn(disc_human=disc_human, disc_anime=disc_anime, gen_anime=gen_anime, gen_human=gen_human, \n",
    "        train_loader_A=train_loader_A, train_loader_B=train_loader_B, \n",
    "        opt_disc=opt_disc, opt_gen=opt_gen, \n",
    "        l1=l1, mse=mse, d_scaler=d_scaler, g_scaler=g_scaler, \n",
    "        num_epochs=remaining_epochs, start_epoch=start_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model_path = get_model_name(\"loss\", epoch=49, learning_rate=LEARNING_RATE, batch_size=BATCH_SIZE)\n",
    "plot_training_curve(gan_model_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 3 - Gesture Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
