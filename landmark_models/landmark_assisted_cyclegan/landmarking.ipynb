{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, fname))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Since there are no labels, we can return a dummy label (0)\n",
    "\n",
    "def create_subset_dataset(original_dir, subset_dir, subset_size=100):\n",
    "    if not os.path.exists(subset_dir):\n",
    "        os.makedirs(subset_dir)\n",
    "    \n",
    "    image_paths = [os.path.join(original_dir, fname) for fname in os.listdir(original_dir) if os.path.isfile(os.path.join(original_dir, fname))]\n",
    "    \n",
    "    np.random.seed(1000)\n",
    "    np.random.shuffle(image_paths)\n",
    "    subset_paths = image_paths[:subset_size]\n",
    "    \n",
    "    for img_path in subset_paths:\n",
    "        shutil.copy(img_path, subset_dir)\n",
    "\n",
    "    print(f\"Subset created with {len(subset_paths)} images in {subset_dir}\")\n",
    "\n",
    "def get_data_loader(data_dir, batch_size, image_size=(128, 128), subset_size=None):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        #transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    assert os.path.exists(data_dir), f\"Directory not found: {data_dir}\"\n",
    "\n",
    "    dataset = CustomImageDataset(root_dir=data_dir, transform=transform)\n",
    "\n",
    "    if subset_size:\n",
    "        num_images = len(dataset)\n",
    "        indices = list(range(num_images))\n",
    "        np.random.seed(1000)\n",
    "        np.random.shuffle(indices)\n",
    "        subset_indices = indices[:subset_size]\n",
    "        subset_sampler = SubsetRandomSampler(subset_indices)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, sampler=subset_sampler)\n",
    "        return loader\n",
    "\n",
    "    def split_indices(dataset):\n",
    "        num_images = len(dataset)\n",
    "        indices = list(range(num_images))\n",
    "        np.random.seed(1000)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_split = int(0.7 * num_images)\n",
    "        val_split = int(0.15 * num_images)\n",
    "\n",
    "        train_indices = indices[:train_split]\n",
    "        val_indices = indices[train_split:train_split + val_split]\n",
    "        test_indices = indices[train_split + val_split:]\n",
    "\n",
    "        return train_indices, val_indices, test_indices\n",
    "\n",
    "    train_indices, val_indices, test_indices = split_indices(dataset)\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load data from the full datasets\n",
    "train_loader_A, val_loader_A, test_loader_A = get_data_loader('/root/aps360-project/data/dataSetA_10k', BATCH_SIZE, image_size=(128, 128))\n",
    "train_loader_B, val_loader_B, test_loader_B = get_data_loader('/root/aps360-project/data/dataSetB_10k', BATCH_SIZE, image_size=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontalface_detector = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture = '/home/alumkalryan/aps360-project/data/dataSetA_10k/000010.jpg'\n",
    "image = cv2.imread(picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_landmarks(image):\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_black = image * 0\n",
    "\n",
    "    rects = frontalface_detector(img_gray, 1)\n",
    "\n",
    "    eye_points = [36,39,42,45]\n",
    "    nose_points = [30]\n",
    "    mouth_points = [48,54]\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = landmark_predictor(img_gray, rect)\n",
    "        shape = [(shape.part(i).x, shape.part(i).y) for i in range(68)]\n",
    "\n",
    "        all_points = eye_points + nose_points + mouth_points\n",
    "        for i in all_points:\n",
    "            x, y = shape[i]\n",
    "            cv2.circle(img_black, (x, y), 1, (255, 255,255), -1)\n",
    "    \n",
    "    return img_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_human_landmarks(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarking Discriminators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landmark Consistency Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkConsistencyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LandmarkConsistencyLoss, self).__init__()\n",
    "        self.landmark_regressor = get_human_landmarks\n",
    "        self.l2_loss = nn.MSELoss() #? correct?\n",
    "    \n",
    "    def forward(self, generated_image, target_image):\n",
    "        generated_landmarks = self.landmark_regressor(generated_image)\n",
    "        target_landmarks = self.landmark_regressor(target_image) #does not use previously saved landmarks\n",
    "        loss = self.l2_loss(generated_landmarks, target_landmarks)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landmark Matched Global Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reusable convolutional block\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, is_downsampling: bool = True, add_activation: bool = True, **kwargs):\n",
    "        super().__init__()\n",
    "        if is_downsampling:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True) if add_activation else nn.Identity(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvolutionalBlock(channels, channels, add_activation=True, kernel_size=3, padding=1, is_downsampling=False),\n",
    "            ConvolutionalBlock(channels, channels, add_activation=False, kernel_size=3, padding=1, is_downsampling=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Define the reusable block\n",
    "class ConvInstanceNormLeakyReLUBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=4,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just change previous discriminator to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two global discriminators, one unconditional and another conditional\n",
    "\n",
    "# Define the discriminator using the reusable block\n",
    "class GlobalDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, pm=64, conditional = False):\n",
    "        super().__init__()\n",
    "        self.conditional = conditional\n",
    "        features = [pm * 1, pm * 2, pm * 4, pm * 8]\n",
    "        \n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                features[0],\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\",\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                ConvInstanceNormLeakyReLUBlock(\n",
    "                    in_channels,\n",
    "                    feature,\n",
    "                    stride=2\n",
    "                )\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        # Adjust the last block to match the original structure\n",
    "        layers.append(\n",
    "            ConvInstanceNormLeakyReLUBlock(\n",
    "                in_channels,\n",
    "                features[-1],\n",
    "                stride=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Final layer\n",
    "        layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=features[-1],\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, landmarks = None):\n",
    "        if self.conditional and landmarks is not None:\n",
    "            x = torch.cat([x, landmarks], dim=1)\n",
    "        x = self.initial_layer(x)\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landmark Guided Local Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LocalDiscriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.main(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "\n",
    "picture = '/home/alumkalryan/aps360-project/data/dataSetB_20k/141_2000.jpg'\n",
    "image = cv2.imread(picture)\n",
    "\n",
    "def detect(image, cascade_file = \"./lbpcascade_animeface.xml\"):\n",
    "    if not os.path.isfile(cascade_file):\n",
    "        raise RuntimeError(\"%s: not found\" % cascade_file)\n",
    "\n",
    "    cascade = cv2.CascadeClassifier(cascade_file)\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.equalizeHist(gray)\n",
    "\n",
    "    faces = cascade.detectMultiScale(gray,\n",
    "                                     # detector options\n",
    "                                     scaleFactor = 1.1,\n",
    "                                     minNeighbors = 5,\n",
    "                                     minSize = (24, 24))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "    plt.imshow(image)\n",
    "\n",
    "detect(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
